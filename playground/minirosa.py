# -*- coding: utf-8 -*-
"""miniRosa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAeZo7opMbpiQh6xlVS3ijNpMf1w59Kd
"""

import numpy as np
from tensorflow import keras

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

data = '/content/drive/MyDrive/cleanedRosa.csv'

rosa = pd.read_csv(data)
rosa

with open(data, encoding='utf-8') as f:
    Corpus = f.readlines()  
Corpus = ' '.join(Corpus).lower().split('\n')

Corpus

print('Corpus Length:', len(Corpus))

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dropout

tokenizer = Tokenizer()
tokenizer.fit_on_texts(Corpus)
totalWords = len(tokenizer.word_index) + 1
print(totalWords)

sequences = []
for line in Corpus:
    tokenList = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(tokenList)):
        ngramSequence = tokenList[:i+1]
        sequences.append(ngramSequence)

sequences[:5]

def padSequences(sequences):
    maxSequenceLen = max([len(seq) for seq in sequences])
    sequences = np.array(pad_sequences(sequences, maxlen=maxSequenceLen, padding='pre'))
    
    predictors, label = sequences[:,:-1], sequences[:,-1]
    label = to_categorical(label, num_classes=totalWords)
    return predictors, label, maxSequenceLen

predictors, label, maxSequenceLen = padSequences(sequences)

model = models.Sequential()
model.add(layers.Embedding(totalWords, 64, input_length=maxSequenceLen - 1))
model.add(layers.LSTM(128))
model.add(layers.Dense(totalWords, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam',
             metrics=['accuracy'])

history = model.fit(predictors, label, epochs=100, callbacks=[EarlyStopping(monitor='loss', patience=10,
                                                                            restore_best_weights=True)])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

accuracy = history.history['accuracy']
loss = history.history['loss']
epochs = range(1, len(accuracy) + 1)

plt.style.use('ggplot')
fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,10))
plot = ax1.plot(epochs, accuracy, 'bo', label='Accuracy')
plot = ax2.plot(epochs, loss, 'bo', label='Loss')
ax1.set(title='Language Model Accuracy', ylabel='Accuracy')
ax2.set(title='Language Model Loss', xlabel='Epochs', ylabel='Loss')

fig.suptitle('Loss/Accuracy of the Language Model', fontsize=16, fontweight = 'bold')

def generateText(seed):   #A text seed is provided
    for wordLength in range(45):   #Generates a text with a range of word length
        tokenList = tokenizer.texts_to_sequences([seed])[0]  #Turns the seed into sequences
        tokenList = pad_sequences([tokenList], maxlen=maxSequenceLen - 1, padding='pre')
        predicted = model.predict_classes(tokenList, verbose=0) #Predicts the next sequence(generated
        outputWord = " "                                         #text)  
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                outputWord = word
                break
        seed += " " + outputWord     #Returns the seed plus generated text
    return seed

generateText('my love')

generateText('Honey')

generateText('Bumble bee')

generateText('Loofu mi')

generateText('mi Love')

generateText('my Heartbeat')

